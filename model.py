"""
Model module for GLUE tasks using PyTorch Lightning.
"""

from datetime import datetime
from typing import Optional

import evaluate
import lightning as L
import torch
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup,
)


class GLUETransformer(L.LightningModule):
    """Lightning Module for GLUE task fine-tuning."""

    def __init__(
            self,
            model_name_or_path: str,
            num_labels: int,
            task_name: str,
            learning_rate: float = 2e-5,
            warmup_steps: int = 0,
            weight_decay: float = 0.0,
            eval_batch_size: int = 32,
            eval_splits: Optional[list] = None,
            optimizer_eps: float = 1e-8,
            **kwargs,
    ):
        super().__init__()
        self.save_hyperparameters()

        # Load model configuration and pretrained model
        self.config = AutoConfig.from_pretrained(
            model_name_or_path,
            num_labels=num_labels
        )
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name_or_path,
            config=self.config
        )

        # Load evaluation metric
        self.metric = evaluate.load(
            "glue",
            self.hparams.task_name,
            experiment_id=datetime.now().strftime("%d-%m-%Y_%H-%M-%S"),
        )

        self.validation_step_outputs = []

    def forward(self, **inputs):
        """Forward pass through the model."""
        return self.model(**inputs)

    def training_step(self, batch, batch_idx):
        """Training step."""
        outputs = self(**batch)
        loss = outputs[0]
        return loss

    def validation_step(self, batch, batch_idx, dataloader_idx=0):
        """Validation step."""
        outputs = self(**batch)
        val_loss, logits = outputs[:2]

        # Get predictions
        if self.hparams.num_labels > 1:
            preds = torch.argmax(logits, axis=1)
        elif self.hparams.num_labels == 1:
            preds = logits.squeeze()

        labels = batch["labels"]

        # Store outputs for epoch end
        self.validation_step_outputs.append({
            "loss": val_loss,
            "preds": preds,
            "labels": labels
        })

        return val_loss

    def on_validation_epoch_end(self):
        """Aggregate validation results at epoch end."""
        # Handle MNLI special case (multiple validation sets)
        if self.hparams.task_name == "mnli":
            for i, output in enumerate(self.validation_step_outputs):
                split = self.hparams.eval_splits[i].split("_")[-1]
                preds = torch.cat([x["preds"] for x in output]).detach().cpu().numpy()
                labels = torch.cat([x["labels"] for x in output]).detach().cpu().numpy()
                loss = torch.stack([x["loss"] for x in output]).mean()

                self.log(f"val_loss_{split}", loss, prog_bar=True)
                split_metrics = {
                    f"{k}_{split}": v
                    for k, v in self.metric.compute(
                        predictions=preds,
                        references=labels
                    ).items()
                }
                self.log_dict(split_metrics, prog_bar=True)

            self.validation_step_outputs.clear()
            return loss

        # Standard case (single validation set)
        preds = torch.cat(
            [x["preds"] for x in self.validation_step_outputs]
        ).detach().cpu().numpy()

        labels = torch.cat(
            [x["labels"] for x in self.validation_step_outputs]
        ).detach().cpu().numpy()

        loss = torch.stack(
            [x["loss"] for x in self.validation_step_outputs]
        ).mean()

        # Log metrics
        self.log("val_loss", loss, prog_bar=True)
        metrics = self.metric.compute(predictions=preds, references=labels)
        self.log_dict(metrics, prog_bar=True)

        self.validation_step_outputs.clear()

    def configure_optimizers(self):
        """Configure optimizer and learning rate scheduler."""
        model = self.model

        # Separate parameters for weight decay
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [
                    p for n, p in model.named_parameters()
                    if not any(nd in n for nd in no_decay)
                ],
                "weight_decay": self.hparams.weight_decay,
            },
            {
                "params": [
                    p for n, p in model.named_parameters()
                    if any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
        ]

        # Create optimizer
        optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters,
            lr=self.hparams.learning_rate,
            eps=self.hparams.optimizer_eps,
        )

        # Create learning rate scheduler with warmup
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.hparams.warmup_steps,
            num_training_steps=self.trainer.estimated_stepping_batches,
        )

        scheduler_config = {
            "scheduler": scheduler,
            "interval": "step",
            "frequency": 1,
        }

        return [optimizer], [scheduler_config]